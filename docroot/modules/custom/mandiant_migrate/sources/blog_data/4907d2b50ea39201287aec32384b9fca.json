{
  "jcr:primaryType": "cq:Page",
  "jcr:createdBy": "admin",
  "jcr:created": "Tue Jun 05 2018 12:40:56 GMT-0400",
  "jcr:content": {
    "jcr:primaryType": "cq:PageContent",
    "jcr:mixinTypes": [
      "mix:versionable"
    ],
    "jcr:createdBy": "admin",
    "jcr:title": "Reverse Engineering the Analyst: Building Machine Learning Models for the SOC",
    "cq:lastReplicationAction": "Activate",
    "jcr:versionHistory": "bd28aab1-90ab-48f4-b939-99eb97e923dd",
    "author": "Matt Berninger",
    "cq:template": "\/apps\/fireeye-blog\/templates\/page_blogpost",
    "cq:lastReplicatedBy": "adam.greenberg@fireeye.com",
    "jcr:language": "en_us",
    "jcr:predecessors": [
      "87c09ee1-92b9-449b-acdc-57331e16983e"
    ],
    "jcr:created": "Tue Jun 05 2018 14:01:47 GMT-0400",
    "cq:lastReplicated": "Tue Jun 05 2018 14:01:47 GMT-0400",
    "cq:lastModified": "Tue Jun 05 2018 14:01:40 GMT-0400",
    "jcr:baseVersion": "87c09ee1-92b9-449b-acdc-57331e16983e",
    "jcr:isCheckedOut": true,
    "cq:tags": [
      "fireeye-blog-authors:matt-berninger",
      "fireeye-blog-authors:awalin-sopan",
      "fireeye-blog-threat-research:threat-research",
      "fireeye-blog-tags:homepage-carousel",
      "fireeye-blog-tags:latest",
      "fireeye-blog-tags:machine-learning",
      "fireeye-blog-tags:soc",
      "fireeye-blog-tags:soc-analyst",
      "fireeye-blog-tags:security-operations-center"
    ],
    "jcr:uuid": "2016c412-b721-4b34-ab2f-9dfa92f76f94",
    "sling:resourceType": "social\/blog\/components\/page",
    "published": "Tue Jun 05 2018 12:30:00 GMT-0400",
    "cq:lastModifiedBy": "adam.greenberg@fireeye.com",
    "par": {
      "jcr:primaryType": "nt:unstructured",
      "sling:resourceType": "foundation\/components\/parsys",
      "entry": {
        "jcr:primaryType": "nt:unstructured",
        "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
        "text": "\u003Cp\u003EMany cyber incidents can be traced back to an original alert that was either missed or ignored by the Security Operations Center (SOC) or Incident Response (IR) team. While most analysts and SOCs are vigilant and responsive, the fact is they are often overwhelmed with alerts. If a SOC is unable to review all the alerts it generates, then sooner or later, something important will slip through the cracks.\u003C\/p\u003E\n\u003Cp\u003EThe core issue here is scalability. It is far easier to create more alerts than to create more analysts, and the cyber security industry is far better at alert generation than resolution. More intel feeds, more tools, and more visibility all add to the flood of alerts. There are things that SOCs can and should do to manage this flood, such as increasing automation of forensic tasks (pulling PCAP and acquiring files, for example) and using aggregation filters to group alerts into similar batches. These are effective strategies and will help reduce the number of required actions a SOC analyst must take. However, the \u003Ci\u003Edecisions\u003C\/i\u003E the SOC makes still form a critical bottleneck. This is the \u201cAnalyze\/ Decide\u201d block in Figure 1.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/MachineLearningSOC\/Fig1.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 1: Basic SOC triage stages\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EIn this blog post, we propose machine learning based strategies to help mitigate this bottleneck and take back control of the SOC. We have implemented these strategies in our FireEye Managed Defense SOC, and our analysts are taking advantage of this approach within their alert triaging workflow. In the following sections, we will describe our process to collect data, capture alert analysis, create a model, and build an efficacy workflow \u2013 all with the ultimate goal of automating alert triage and freeing up analyst time.\u003C\/p\u003E\n\u003Ch4\u003EReverse Engineering the Analyst\u003C\/h4\u003E\n\u003Cp\u003EEvery alert that comes into a SOC environment contains certain bits of information that an analyst uses to determine if the alert represents malicious activity. Often, there are well-paved analytical processes and pathways used when evaluating these forensic artifacts over time. We wanted to explore if, in an effort to truly scale our SOC operations, we could extract these analytical pathways, train a machine to traverse them, and potentially discover new ones.\u003C\/p\u003E\n\u003Cp\u003EThink of a SOC as a self-contained machine that inputs unlabeled alerts and outputs the alerts labeled as \u201cmalicious\u201d or \u201cbenign\u201d. How can we capture the analysis and determine that something is indeed malicious, and then recreate that analysis at scale? In other words, what if we could train a machine to make the same analytical decisions as an analyst, within an acceptable level of confidence?\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EBasic Supervised Model Process\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EThe data science term for this is a \u201cSupervised Classification Model\u201d. It is \u201csupervised\u201d in the sense that it learns by being shown data already labeled as benign or malicious, and it is a \u201cclassification model\u201d in the sense that once it has been trained, we want it to look at a new piece of data and make a decision between one of several discrete outcomes. In our case, we only want it to decide between two \u201cclasses\u201d of alerts: malicious and benign.\u003C\/p\u003E\n\u003Cp\u003EIn order to begin creating such a model, a dataset must be collected. This dataset forms the \u201cexperience\u201d of the model, and is the information we will use to \u201ctrain\u201d the model to make decisions. In order to supervise the model, each unit of data must be labeled as either malicious or benign, so that the model can evaluate each observation and begin to figure out what makes something malicious versus what makes it benign. Typically, collecting a clean, labeled dataset is one of the hardest parts of the supervised model pipeline; however, in the case of our SOC, our analysts are constantly triaging (or \u201clabeling\u201d) thousands of alerts every week, and so we were lucky to have an abundance of clean, standardized, labeled alerts.\u003C\/p\u003E\n\u003Cp\u003EOnce a labeled dataset has been defined, the next step is to define \u201cfeatures\u201d that can be used to portray the information resident in each alert. A \u201cfeature\u201d can be thought of as an aspect of a bit of information. For example, if the information is represented as a string, a natural \u201cfeature\u201d could be the length of the string. The central idea behind building features for our alert classification model was to find a way to represent and record all the aspects that an analyst might consider when making a decision.\u003C\/p\u003E\n\u003Cp\u003EBuilding the model then requires choosing a model structure to use, and training the model on a subset of the total data available. The larger and more diverse the training data set, generally the better the model will perform. The remaining data is used as a \u201ctest set\u201d to see if the trained model is indeed effective. Holding out this test set ensures the model is evaluated on samples it has never seen before, but for which the true labels are known.\u003C\/p\u003E\n\u003Cp\u003EFinally, it is critical to ensure there is a way to evaluate the efficacy of the model over time, as well as to investigate mistakes so that appropriate adjustments can be made. Without a plan and a pipeline to evaluate and retrain, the model will almost certainly decay in performance.\u003C\/p\u003E\n\u003Ch4\u003EFeature Engineering\u003C\/h4\u003E\n\u003Cp\u003EBefore creating any of our own models, we interviewed experienced analysts and documented the information they typically evaluate before making a decision on an alert. Those interviews formed the basis of our feature extraction. For example, when an analyst says that reviewing an alert is \u201ceasy\u201d, we ask: \u201cWhy? And what helps you make that decision?\u201d It is this reverse engineering of sorts that gives insight into features and models we can use to capture analysis.\u003C\/p\u003E\n\u003Cp\u003EFor example, consider a process execution event. An alert on a potentially malicious process execution may contain the following fields:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003EProcess Path\u003C\/li\u003E\n\u003Cli\u003EProcess MD5\u003C\/li\u003E\n\u003Cli\u003EParent Process\u003C\/li\u003E\n\u003Cli\u003EProcess Command Arguments\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EWhile this may initially seem like a limited feature space, there is a lot of useful information that one can extract from these fields.\u003C\/p\u003E\n\u003Cp\u003EBeginning with the process path of, say, \u201cC:\\windows\\temp\\m.exe\u201d, an analyst can immediately see some features:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003EThe process resides in a temporary folder: C:\\windows\\temp\\\u003C\/li\u003E\n\u003Cli\u003EThe process is two directories deep in the file system\u003C\/li\u003E\n\u003Cli\u003EThe process executable name is one character long\u003C\/li\u003E\n\u003Cli\u003EThe process has an .exe extension\u003C\/li\u003E\n\u003Cli\u003EThe process is not a \u201ccommon\u201d process name\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EWhile these may seem simple, over a vast amount of data and examples, extracting these bits of information will help the model to differentiate between events. Even the most basic aspects of an artifact must be captured in order to \u201cteach\u201d the model to view processes the way an analyst does.\u003C\/p\u003E\n\u003Cp\u003EThe features are then encoded into a more discrete representation, similar to this:\u003C\/p\u003E\n\u003Ctable border=\u00220\u0022 cellspacing=\u00220\u0022 cellpadding=\u00220\u0022 width=\u0022543\u0022\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd width=\u0022115\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003ETemp_folder\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003EDepth\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003EName_Length\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003EExtension\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u0022168\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003Ecommon_process_name\u003C\/p\u003E\n\u003C\/td\u003E\n\u003C\/tr\u003E\u003Ctr\u003E\u003Ctd width=\u0022115\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003ETRUE\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003E2\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003E1\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u002287\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003Eexe\u003C\/p\u003E\n\u003C\/td\u003E\n\u003Ctd width=\u0022168\u0022 valign=\u0022bottom\u0022\u003E\u003Cp\u003EFALSE\u003C\/p\u003E\n\u003C\/td\u003E\n\u003C\/tr\u003E\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Cp\u003EAnother important feature to consider about a process execution event is the combination of parent process and child process. Deviation from expected \u201clineage\u201d can be a strong indicator of malicious activity.\u003C\/p\u003E\n\u003Cp\u003ESay the parent process of the aforementioned example was \u2018powershell.exe\u2019. Potential new features could then be derived from the concatenation of the parent process and the process itself: \u2018powershell.exe_m.exe\u2019. This functionally serves as an identity for the parent-child relation and captures another key analysis artifact.\u003C\/p\u003E\n\u003Cp\u003EThe richest field, however, is probably the process arguments. Process arguments are their own sort of language, and language analysis is a well-tread space in predictive analytics.\u003C\/p\u003E\n\u003Cp\u003EWe can look for things including, but not limited to:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003ENetwork connection strings (such as \u2018http:\/\/\u2019, \u2018https:\/\/\u2019, \u2018ftp:\/\/\u2019).\u003C\/li\u003E\n\u003Cli\u003EBase64 encoded commands\u003C\/li\u003E\n\u003Cli\u003EReference to Registry Keys (\u2018HKLM\u2019, \u2018HKCU\u2019)\u003C\/li\u003E\n\u003Cli\u003EEvidence of obfuscation (ticks, $, semicolons) (read \u003Ca href=\u0022https:\/\/www.fireeye.com\/blog\/threat-research\/2018\/03\/dosfuscation-exploring-obfuscation-and-detection-techniques.html\u0022\u003EDaniel Bohannon\u2019s work\u003C\/a\u003E for more)\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EThe way these features and their values appear in a training dataset will define the way the model learns. Based on the distribution of features across thousands of alerts, relationships will start to emerge between features and labels. These relationships will then be recorded in our model, and ultimately used to influence the predictions for new alerts. Looking at distributions of features in the training set can give insight into some of these potential relationships.\u003C\/p\u003E\n\u003Cp\u003EFor example, Figure 2 shows how the distribution of Process Command Length may appear when grouping by malicious (red) and benign (blue).\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/MachineLearningSOC\/Fig2.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 2: Distribution of Process Event alerts grouped by Process Command Length\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EThis graph shows that over a subset of samples, the longer the command length, the more likely it is to be malicious. This manifests as red on the right and blue on the left. However, process length is not the only factor.\u003C\/p\u003E\n\u003Cp\u003EAs part of our feature set, we also thought it would be useful to approximate the \u201ccomplexity\u201d of each command. For this, we used \u201c\u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Entropy_(information_theory)\u0022\u003EShannon entropy\u003C\/a\u003E\u201d, a commonly used metric that measures the degree of randomness present in a string of characters.\u003C\/p\u003E\n\u003Cp\u003EFigure 3 shows a distribution of command entropy, broken out into malicious and benign. While the classes do not separate entirely, we can see that for this sample of data, samples with higher entropy generally have a higher chance of being malicious.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/MachineLearningSOC\/Fig3.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 3: Distribution of Process Event alerts grouped by entropy\u003C\/span\u003E\u003C\/p\u003E\n\u003Ch4\u003EModel Selection and Generalization\u003C\/h4\u003E\n\u003Cp\u003EOnce features have been generated for the whole dataset, it is time to use them to train a model. There is no perfect procedure for picking the best model, but looking at the type of features in our data can help narrow it down. In the case of a process event, we have a combination of features represented as strings and numbers. When an analyst evaluates each artifact, they ask questions about each of these features, and combine the answers to estimate the probability that the process is malicious.\u003C\/p\u003E\n\u003Cp\u003EFor our use case, it also made sense to prioritize an \u2018interpretable\u2019 model \u2013 that is, one that can more easily expose why it made a certain decision about an artifact. This way analysts can build confidence in the model, as well as detect and fix analytical mistakes that the model is making. Given the nature of the data, the decisions analysts make, and the desire for interpretability, we felt that a decision tree-based model would be well-suited for alert classification.\u003C\/p\u003E\n\u003Cp\u003EThere are many publicly available resources to learn about decision trees, but the basic intuition behind a decision tree is that it is an iterative process, asking a series of questions to try to arrive at a highly confident answer. Anyone who has played the game \u201cTwenty Questions\u201d is familiar with this concept. Initially, general questions are asked to help eliminate possibilities, and then more specific questions are asked to narrow down the possibilities. After enough questions are asked and answered, the \u2018questioner\u2019 feels they have a high probability of guessing the right answer.\u003C\/p\u003E\n\u003Cp\u003EFigure 4 shows an example of a decision tree that one might use to evaluate process executions.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/MachineLearningSOC\/Fig4.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 4: Decision tree for deciding whether an alert is benign or malicious\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EFor the example alert in the diagram, the \u201cdecision path\u201d is marked in red. This is how this decision tree model makes a prediction. It first asks: \u201cIs the length greater than 100 characters?\u201d If so, it moves to the next question \u201cDoes it contain the string \u2018http\u2019?\u201d and so on until it feels confident in making an educated guess. In the example in Figure 4, given that 95 percent of all the training alerts traveling this decision path were malicious, the model predicts a 95 percent chance that this alert will also be malicious.\u003C\/p\u003E\n\u003Cp\u003EBecause they can ask such detailed combinations of questions, it is possible that decision trees can \u201coverfit\u201d, or learn rules that are too closely tied to the training set. This reduces the model\u2019s ability to \u201cgeneralize\u201d to new data. One way to mitigate this effect is to use many slightly different decision trees and have them each \u201cvote\u201d on the outcome. This \u201censemble\u201d of decision trees is called a Random Forest, and it can improve performance for the model when deployed in the wild. This is the algorithm we ultimately chose for our model.\u003C\/p\u003E\n\u003Ch4\u003EHow the SOC Alert Model Works\u003C\/h4\u003E\n\u003Cp\u003EWhen a new alert appears, the data in the artifact is transformed into a vector of the encoded features, with the same structure as the feature representations used to train the model. The model then evaluates this \u201cfeature vector\u201d and applies a confidence level for the predicted label. Based on thresholds we set, we can then classify the alert as malicious or benign.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/MachineLearningSOC\/Fig5.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 5: An alert presented to the analyst with its raw values captured\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EAs an example, the event shown in Figure 5 might create the following feature values:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003EParent Process: \u2018wscript\u2019\u003C\/li\u003E\n\u003Cli\u003ECommand Entropy: 5.08\u003C\/li\u003E\n\u003Cli\u003ECommand Length =103\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EBased on how they were trained, the trees in the model each ask a series of questions of the new feature vector. As the feature vector traverses each tree, it eventually converges on a terminal \u201cleaf\u201d classifying it as either benign or malicious. We can then evaluate the aggregated decisions made by each tree to estimate which features in the vector played the largest role in the ultimate classification.\u003C\/p\u003E\n\u003Cp\u003EFor the analysts in the SOC, we then present the features extracted from the model, showing the distribution of those features over the entire dataset. This gives the analysts insight into \u201cwhy\u201d the model thought what it thought, and how those features are represented across all alerts we have seen. For example, the \u201cexplanation\u201d for this alert might look like:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003ECommand Entropy = 5.08 \u0026gt; 4.60: \u0026nbsp;51.73% Threat\u003C\/li\u003E\n\u003Cli\u003EoccuranceOfChar \u201c\\\u201d= 9.00 \u0026gt; 4.50:\u0026nbsp; 64.09% Threat\u003C\/li\u003E\n\u003Cli\u003EoccuranceOfChar:\u201c)\u201d (=0.00) \u0026lt;= 0.50: 78.69% Threat\u003C\/li\u003E\n\u003Cli\u003ENOT processTree=\u201dcmd.exe_to_cscript.exe\u201d: 99.6% Threat\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EThus, at the time of analysis, the analysts can see the raw data of the event, the prediction from the model, an approximation of the decision path, and a simplified, interpretable view of the overall feature importance.\u003C\/p\u003E\n\u003Ch4\u003EHow the SOC Uses the Model\u003C\/h4\u003E\n\u003Cp\u003EShowing the features the model used to reach the conclusion allows experienced analysts to compare their approach with the model, and give feedback if the model is doing something wrong. Conversely, a new analyst may learn to look at features they may have otherwise missed: the parent-child relationship, signs of obfuscation, or network connection strings in the arguments. After all, the model has learned on the collective experience of every analyst over thousands of alerts. Therefore, the model provides an actionable reflection of the aggregate analyst experience \u003Ci\u003Eback\u003C\/i\u003E to the SOC, so that each analyst can transitively learn from their colleagues.\u003C\/p\u003E\n\u003Cp\u003EAdditionally, it is possible to write rules using the output of the model as a parameter. If the model is particularly confident on a subset of alerts, and the SOC feels comfortable automatically classifying that family of threats, it is possible to simply write a rule to say: \u201cIf the alert is of this type, AND for this malware family, AND the model confidence is above 99, automatically call this alert bad and generate a report.\u201d Or, if there is a storm of probable false positives, one could write a rule to cull the herd of false positives using a model score below 10.\u003C\/p\u003E\n\u003Ch4\u003EHow the Model Stays Effective\u003C\/h4\u003E\n\u003Cp\u003EThe day the model is trained, it stops learning. However, threats \u2013 and therefore alerts \u2013 are constantly evolving. Thus, it is imperative to continually retrain the model with new alert data to ensure it continues to learn from changes in the environment.\u003C\/p\u003E\n\u003Cp\u003EAdditionally, it is critical to monitor the overall efficacy of the model over time. Building an efficacy analysis pipeline to compare model results against analyst feedback will help identify if the model is beginning to drift or develop structural biases. Evaluating and incorporating analyst feedback is also critical to identify and address specific misclassifications, and discover potential new features that may be necessary.\u003C\/p\u003E\n\u003Cp\u003ETo accomplish these goals, we run a background job that updates our training database with newly labeled events. As we get more and more alerts, we periodically retrain our model with the new observations. If we encounter issues with accuracy, we diagnose and work to address them. Once we are satisfied with the overall accuracy score of our retrained model, we store the model object and begin using that model version.\u003C\/p\u003E\n\u003Cp\u003EWe also provide a feedback mechanism for analysts to record when the model is wrong. An analyst can look at the label provided by the model and the explanation, but can also make their own decision. Whether they agree with the model or not, they can input their own label through the interface. We store this label provided by the analyst along with any optional explanation given by them regarding the explanation.\u003C\/p\u003E\n\u003Cp\u003EFinally, it should be noted that these manual labels may require further evaluation. As an example, consider a commodity malware alert, in which network command and control communications were sinkholed. An analyst may evaluate the alert, pull back triage details, including PCAP samples, and see that while the malware executed, the true \u003Ci\u003Ethreat\u003C\/i\u003E to the environment was mitigated. Since it does not represent an exigent threat, the analyst may mark this alert as \u2018benign\u2019. However, the fact that it was sinkholed does not change that the artifacts of execution still represent malicious activity. Under different circumstances, this infection could have had a negative impact on the organization. However, if the benign label is used when retraining the model, that will teach the model that something inherently malicious is in fact benign, and potentially lead to false negatives in the future.\u003C\/p\u003E\n\u003Cp\u003EMonitoring efficacy over time, updating and retraining the model with new alerts, and evaluating manual analyst feedback gives us visibility into how the model is performing and learning over time. Ultimately this helps to build confidence in the model, so we can automate more tasks and free up analyst time to perform tasks such as hunting and investigation.\u003C\/p\u003E\n\u003Ch4\u003EConclusion\u003C\/h4\u003E\n\u003Cp\u003EA supervised learning model is not a replacement for an experienced analyst. However, incorporating predictive analytics and machine learning into the SOC workflow can help augment the productivity of analysts, free up time, and ensure they utilize investigative skills and creativity on the threats that truly require expertise.\u003C\/p\u003E\n\u003Cp\u003EThis blog post outlines the major components and considerations of building an alert classification model for the SOC. Data collection, labeling, feature generation, model training, and efficacy analysis must all be carefully considered when building such a model. FireEye continues to iterate on this research to improve our detection and response capabilities, continually improve the detection efficacy of our products, and ultimately protect our clients.\u003C\/p\u003E\n\u003Cp\u003EThe process and examples shown discussed in this post are not mere research. Within our \u003Ca href=\u0022https:\/\/www.fireeye.com\/solutions\/managed-defense.html\u0022\u003EFireEye Managed Defense\u003C\/a\u003E SOC, we use alert classification models built using the aforementioned processes to increase our efficiency and ensure we apply our analysts\u2019 expertise where it is needed most. In a world of ever increasing threats and alerts, increasing SOC efficiency may mean the difference between missing and catching a critical intrusion.\u003C\/p\u003E\n\u003Ch4\u003EAcknowledgements\u003C\/h4\u003E\n\u003Cp\u003EA big thank you to Seth Summersett and Clara Brooks.\u003C\/p\u003E\n\u003Cp\u003E***\u003C\/p\u003E\n\u003Cp\u003EThe FireEye ICE Data Science Team is a small, highly trained team of data scientists and engineers, focused on delivering impactful capabilities to our analysts, products, and customers. ICE-DS is always looking for exceptional candidates interested in researching and solving difficult problems in cybersecurity. If you\u2019re interested, check out\u0026nbsp;\u003Cu\u003E\u003Ca href=\u0022https:\/\/www.fireeye.com\/company\/jobs.html\u0022\u003EFireEye careers\u003C\/a\u003E\u003C\/u\u003E.\u003C\/p\u003E\n",
        "jcr:lastModified": "Tue Jun 05 2018 14:01:40 GMT-0400",
        "sling:resourceType": "social\/blog\/components\/entrytext"
      }
    },
    "summary": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "text": "\u003Cp\u003EWe\u0026nbsp;describe machine learning based strategies to collect data, capture alert analysis, create a model, and build an efficacy workflow \u2013 all with the ultimate goal of automating alert triage and freeing up analyst time.\u003C\/p\u003E\n",
      "jcr:lastModified": "Tue Jun 05 2018 12:40:31 GMT-0400",
      "sling:resourceType": "social\/blog\/components\/entrytextteaser"
    },
    "image": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "jcr:lastModified": "Tue Jun 05 2018 13:34:07 GMT-0400",
      "imageRotate": "0"
    }
  }
}
