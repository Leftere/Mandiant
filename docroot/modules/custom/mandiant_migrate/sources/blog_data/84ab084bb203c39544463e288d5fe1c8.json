{
  "jcr:primaryType": "cq:Page",
  "jcr:createdBy": "admin",
  "jcr:created": "Wed May 29 2019 10:30:32 GMT-0400",
  "jcr:content": {
    "jcr:primaryType": "cq:PageContent",
    "jcr:mixinTypes": [
      "mix:versionable"
    ],
    "jcr:createdBy": "admin",
    "jcr:title": "Learning to Rank Strings Output for Speedier Malware Analysis",
    "cq:lastReplicationAction": "Activate",
    "jcr:versionHistory": "48f53c71-a6e4-4918-81be-358d04a0b39e",
    "author": "Philip Tully",
    "cq:template": "\/apps\/fireeye-blog\/templates\/page_blogpost",
    "cq:lastReplicatedBy": "adam.greenberg@fireeye.com",
    "jcr:language": "en_us",
    "jcr:predecessors": [
      "5d16875a-f221-436c-bc79-843364a4bc37"
    ],
    "jcr:created": "Wed May 29 2019 10:30:32 GMT-0400",
    "cq:lastReplicated": "Wed May 29 2019 10:30:25 GMT-0400",
    "cq:lastModified": "Wed May 29 2019 10:30:05 GMT-0400",
    "jcr:baseVersion": "5d16875a-f221-436c-bc79-843364a4bc37",
    "jcr:isCheckedOut": true,
    "cq:tags": [
      "fireeye-blog-authors:philip-tully",
      "fireeye-blog-authors:matthew-haigh",
      "fireeye-blog-authors:jay-gibble",
      "fireeye-blog-authors:cap-michael-sikorski",
      "fireeye-blog-threat-research:threat-research",
      "fireeye-blog-tags:FLARE",
      "fireeye-blog-tags:homepage-carousel",
      "fireeye-blog-tags:latest",
      "fireeye-blog-tags:machine-learning",
      "fireeye-blog-tags:malware",
      "fireeye-blog-tags:malware-analysis"
    ],
    "jcr:uuid": "60d63743-6a10-4acb-bc94-64361db797ea",
    "sling:resourceType": "social\/blog\/components\/page",
    "published": "Wed May 29 2019 10:30:00 GMT-0400",
    "cq:lastModifiedBy": "adam.greenberg@fireeye.com",
    "par": {
      "jcr:primaryType": "nt:unstructured",
      "sling:resourceType": "foundation\/components\/parsys",
      "entry": {
        "jcr:primaryType": "nt:unstructured",
        "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
        "text": "\u003Cp\u003EReverse engineers, forensic investigators, and incident responders have an arsenal of tools at their disposal to dissect malicious software binaries. When performing malware analysis, they successively apply these tools in order to gradually gather clues about a binary\u2019s function, design detection methods, and ascertain how to contain its damage. One of the most useful initial steps is to inspect its printable characters via the \u003Ca href=\u0022https:\/\/docs.microsoft.com\/en-us\/sysinternals\/downloads\/strings\u0022\u003E\u003Ci\u003EStrings \u003C\/i\u003Eprogram\u003C\/a\u003E. A binary will often contain strings if it performs operations like printing an error message, connecting to a URL, creating a registry key, or copying a file to a specific location \u2013 each of which provide crucial hints that can help drive future analysis.\u003C\/p\u003E\n\u003Cp\u003EManually filtering out these relevant strings can be time consuming and error prone, especially considering that:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003ERelevant strings occur disproportionately less often than irrelevant strings.\u003C\/li\u003E\n\u003Cli\u003ELarger binaries can output upwards of tens of thousands of individual strings.\u003C\/li\u003E\n\u003Cli\u003EThe definition of \u0026quot;relevant\u201d can vary significantly across individual human analysts.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003EInvestigators would never want to miss an important clue that could have reduced their time spent performing the malware analysis, or even worse, led them to draw incomplete or incorrect conclusions. In this blog post, we will demonstrate how the FireEye Data Science (FDS) and FireEye Labs Reverse Engineering (FLARE) teams recently collaborated to streamline this analyst pain point using machine learning.\u003C\/p\u003E\n\u003Ctable cellpadding=\u00221\u0022 cellspacing=\u00220\u0022 border=\u00221\u0022\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cp\u003E\u003Cu\u003EHighlights\u003C\/u\u003E\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003ERunning the \u003Ci\u003EStrings\u003C\/i\u003E program on a piece of malware inevitably produces noisy strings mixed in with important ones, which can only be uncovered after sifting and scrolling through the entirety of its messy output. FireEye\u2019s new machine learning model that automatically ranks strings based on their relevance for malware analysis speeds up this process at scale.\u003C\/li\u003E\n\u003Cli\u003EKnowing which individual strings are relevant often requires highly experienced analysts. Quality, security-relevant labeled training data can be time consuming and expensive to obtain, but weak supervision that leverages the domain expertise of reverse engineers helps accelerate this bottleneck.\u003C\/li\u003E\n\u003Cli\u003EOur proposed learning-to-rank model can efficiently prioritize \u003Ci\u003EStrings \u003C\/i\u003Eoutputs from individual malware samples. On a dataset of relevant strings from over 7 years of malware reports authored by FireEye reverse engineers, it also performs well based on criteria commonly used to evaluate recommendation and search engines.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003C\/td\u003E\n\u003C\/tr\u003E\u003C\/tbody\u003E\u003C\/table\u003E\n\u003Ch4\u003EBackground\u003C\/h4\u003E\n\u003Cp\u003EEach string returned by the \u003Ci\u003EStrings \u003C\/i\u003Eprogram is represented by sequences of 3 characters or more ending with a null terminator, independent of any surrounding context and file formatting. These loose criteria mean that \u003Ci\u003EStrings\u003C\/i\u003E may identify sequences of characters as strings when they are not human-interpretable. For example, if consecutive bytes 0x31, 0x33, 0x33, 0x37, 0x00 appear within a binary, \u003Ci\u003EStrings\u003C\/i\u003E will interpret this as \u201c1337.\u201d However, those ASCII characters may not actually represent that string per se; they could instead represent a memory address, CPU instructions, or even data utilized by the program.\u003Ci\u003E Strings\u003C\/i\u003E leaves it up to the analyst to filter out such irrelevant strings that appear within its output. For instance, only a handful of the strings listed in Figure 1 that originate from an example malicious binary are relevant from a malware analyst\u2019s point of view.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/strings\/Fig1.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 1: An example Strings output containing 44 strings for a toy sample with a SHA-256 value of \u003Ca href=\u0022https:\/\/www.virustotal.com\/#\/file\/eb84360ca4e33b8bb60df47ab5ce962501ef3420bc7aab90655fd507d2ffcedd\/detection\u0022\u003Eeb84360ca4e33b8bb60df47ab5ce962501ef3420bc7aab90655fd507d2ffcedd\u003C\/a\u003E.\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003ERanking strings in terms of descending relevance would make an analyst\u2019s life much easier. They would then only need to focus their attention on the most relevant strings located towards the top of the list, and simply disregard everything below. However, solving the task of automatically ranking strings is not trivial. The space of relevant strings is unstructured and vast, and devising finely tuned rules to robustly account for all the possible variations among them would be a tall order.\u003C\/p\u003E\n\u003Ch4\u003ELearning to Rank \u003Ci\u003EStrings\u003C\/i\u003E Output\u003C\/h4\u003E\n\u003Cp\u003EThis task can instead be formulated in a machine learning (ML) framework called \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Learning_to_rank\u0022\u003Elearning to rank (LTR)\u003C\/a\u003E, which has been historically applied to problems like information retrieval, machine translation, web search, and collaborative filtering. One way to tackle LTR problems is by using \u003Ca href=\u0022https:\/\/papers.nips.cc\/paper\/3270-mcrank-learning-to-rank-using-multiple-classification-and-gradient-boosting.pdf\u0022\u003EGradient Boosted Decision Trees (GBDTs)\u003C\/a\u003E. GBDTs successively learn individual decision trees that reduce the loss using a gradient descent procedure, and ultimately use a weighted sum of every trees\u2019 prediction as an ensemble. GBDTs with an LTR objective function can learn class probabilities to compute each string\u2019s expected relevance, which can then be used to rank a given \u003Ci\u003EStrings \u003C\/i\u003Eoutput. We provide a high-level overview of how this works in Figure 2.\u003C\/p\u003E\n\u003Cp\u003EIn the initial \u003Cspan class=\u0022code\u0022\u003Etrain()\u003C\/span\u003E step of Figure 2, over 25 thousand binaries are run through the \u003Ci\u003EStrings \u003C\/i\u003Eprogram to generate training data consisting of over 18 million total strings\u003Ci\u003E. \u003C\/i\u003EEach\u003Ci\u003E \u003C\/i\u003Etraining sample then corresponds to the concatenated list of ASCII and Unicode strings output by the \u003Ci\u003EStrings \u003C\/i\u003Eprogram on that input file. To train the model, these raw strings are transformed into numerical vectors containing natural language processing features like Shannon entropy and character co-occurrence frequencies, together with domain-specific signals like the presence of indicators of compromise (e.g. file paths, IP addresses, URLs, etc.), format strings, imports, and other relevant landmarks.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/strings\/Fig2.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 2: The ML-based LTR framework ranks strings based on their relevance for malware analysis. This figure illustrates different steps of the machine learning modeling process: the initial train() step is denoted by solid arrows and boxes, and the subsequent predict() and sort() steps are denoted by dotted arrows and boxes.\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EEach transformed string\u2019s feature vector is associated with a non-negative integer label that represents their relevance for malware analysis. Labels range from 0 to 7, with higher numbers indicating increased relevance. To generate these labels, we leverage the subject matter knowledge of FLARE analysts to apply heuristics and impose high-level constraints on the resulting label distributions. While this \u003Ca href=\u0022https:\/\/hazyresearch.github.io\/snorkel\/blog\/weak_supervision.html\u0022\u003Eweak supervision approach\u003C\/a\u003E may generate noise and spurious errors compared to an ideal case where every string is manually labeled, it also provides an inexpensive and model-agnostic way to integrate domain expertise directly into our GBDT model.\u003C\/p\u003E\n\u003Cp\u003ENext during the \u003Cspan class=\u0022code\u0022\u003Epredict()\u003C\/span\u003E step of Figure 2, we use the trained GBDT model to predict ranks for the strings belonging to an input file that was not originally part of the training data, and in this example query we use the \u003Ci\u003EStrings\u003C\/i\u003E output shown in Figure 1. The model predicts ranks for each string in the query as floating-point numbers that represent expected relevance scores, and in the final \u003Cspan class=\u0022code\u0022\u003Esort()\u003C\/span\u003E step of Figure 2, strings are sorted in descending order by these scores. Figure 3 illustrates how this resulting prediction achieves the desired goal of ranking strings according to their relevance for malware analysis.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/strings\/Fig3.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 3: The resulting ranking on the strings depicted in both Figure 1 and in the truncated query of Figure 2. Contrast the relative ordering of the strings shown here to those otherwise identical lists.\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EThe predicted and sorted string rankings in Figure 3 show network-based indicators on top of the list, followed by registry paths and entries. These reveal the potential C2 server and malicious behavior on the host. The subsequent output consisting of user-related information is more likely to be benign, but still worthy of investigation. Rounding out the list are common strings like Windows API functions and PE artifacts that tend to raise no red flags for the malware analyst.\u003C\/p\u003E\n\u003Ch4\u003EQuantitative Evaluation\u003C\/h4\u003E\n\u003Cp\u003EWhile it seems like the model qualitatively ranks the above strings as expected, we would like some quantitative way to assess the model\u2019s performance more holistically. What evaluation criteria can we use to convince ourselves that the model generalizes beyond the coverage of our weak supervision sources, and to compare models that are trained with different parameters?\u003C\/p\u003E\n\u003Cp\u003EWe turn to the recommender systems literature, which uses the \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Discounted_cumulative_gain\u0022\u003ENormalized Discounted Cumulative Gain\u003C\/a\u003E (NDCG) score to evaluate ranking of items (i.e. individual strings) in a collection (i.e. a \u003Ci\u003EStrings\u003C\/i\u003E output). NDCG sounds complicated, but let\u2019s boil it down one letter at a time:\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003E\u201cG\u201d is for gain, which corresponds to the magnitude of each string\u2019s relevance.\u003C\/li\u003E\n\u003Cli\u003E\u201cC\u201d is for cumulative, which refers to the cumulative gain or summed total of every string\u2019s relevance.\u003C\/li\u003E\n\u003Cli\u003E\u201cD\u201d is for discounted, which divides each string\u2019s predicted relevance by a monotonically increasing function like the logarithm of its ranked position, reflecting the goal of having the most relevant strings ranked towards the top of our predictions.\u003C\/li\u003E\n\u003Cli\u003E\u201cN\u201d is for normalized, which means dividing DCG scores by ideal DCG scores calculated for a ground truth holdout dataset, which we obtain from FLARE-identified relevant strings contained within historical malware reports. Normalization makes it possible to compare scores across samples since the number of strings within different \u003Ci\u003EStrings\u003C\/i\u003E outputs can vary widely.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/strings\/Fig4.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 4: Kernel Density Estimate of NDCG@100 scores for Strings outputs from the holdout dataset. Scores are calculated for the original ordering after simply running the Strings program on each binary (gray) versus the predicted ordering from the trained GBDT model (red).\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003EIn practice, we take the first \u003Ci\u003Ek\u003C\/i\u003E strings indexed by their ranks within a single \u003Ci\u003EStrings\u003C\/i\u003E output, where the \u003Ci\u003Ek\u003C\/i\u003E parameter is chosen based on how many strings a malware analyst will attend to or deem relevant on average. For our purposes we set \u003Ci\u003Ek = \u003C\/i\u003E100 based on the approximate average number of relevant strings per \u003Ci\u003EStrings\u003C\/i\u003E output. NDCG@\u003Ci\u003Ek\u003C\/i\u003E scores are bounded between 0 and 1,\u003Ci\u003E \u003C\/i\u003Ewith scores closer to 1 indicating better prediction quality in which more relevant strings surface towards the top. This measurement allows us to evaluate the predictions from a given model versus those generated by other models and ranked with different algorithms.\u003C\/p\u003E\n\u003Cp\u003ETo quantitatively assess model performance, we run the strings from each sample that have ground truth FLARE reports though the \u003Cspan class=\u0022code\u0022\u003Epredict()\u003C\/span\u003E step of Figure 2, and compare their predicted ranks with a baseline of the original ranking of strings output by \u003Ci\u003EStrings\u003C\/i\u003E. The divergence in distributions of NDCG@100 scores between these two approaches demonstrates that the trained GBDT model learns a useful structure that generalizes well to the independent holdout set (Figure 4).\u003C\/p\u003E\n\u003Ch4\u003EConclusion\u003C\/h4\u003E\n\u003Cp\u003EIn this blog post, we introduced an ML model that learns to rank strings based on their relevance for malware analysis. Our results illustrate that it can rank \u003Ci\u003EStrings\u003C\/i\u003E output based both on qualitative inspection (Figure 3) and quantitative evaluation of NDCG@\u003Ci\u003Ek\u003C\/i\u003E (Figure 4). Since \u003Ci\u003EStrings\u003C\/i\u003E is so commonly applied during malware analysis at FireEye and elsewhere, this model could significantly reduce the overall time required to investigate suspected malicious binaries at scale. We plan on continuing to improve its NDCG@\u003Ci\u003Ek\u003C\/i\u003E\u0026nbsp;scores by training it with more high fidelity labeled data, incorporating more sophisticated modeling and featurization techniques, and soliciting further analyst feedback from field testing.\u003C\/p\u003E\n\u003Cp\u003EIt\u2019s well known that malware authors go through great lengths to conceal useful strings from analysts, and a potential blind spot to consider for this model is that the utility of \u003Ci\u003EStrings\u003C\/i\u003E itself can be thwarted by obfuscation. However, open source tools like the \u003Ca href=\u0022https:\/\/www.fireeye.com\/blog\/threat-research\/2016\/06\/automatically-extracting-obfuscated-strings.html\u0022\u003EFireEye Labs Obfuscated Strings Solver (FLOSS)\u003C\/a\u003E can be used as an in-line replacement for \u003Ci\u003EStrings\u003C\/i\u003E. FLOSS automatically extracts printable strings just as \u003Ci\u003EStrings\u003C\/i\u003E does, but additionally reveals obfuscated strings that have been encoded, packed, or manually constructed on the stack. The model can be readily trained on FLOSS outputs to rank even obfuscated strings. Furthermore, since it can be applied to arbitrary lists of strings, the model could also be used to rank strings extracted from live memory dumps and sandbox runs.\u003C\/p\u003E\n\u003Cp\u003EThis work represents a collaboration between the FDS and FLARE teams, which together build \u003Ca href=\u0022https:\/\/www.fireeye.com\/blog\/products-and-services\/2018\/07\/malwareguard-fireeye-machine-learning-model-to-detect-and-prevent-malware.html\u0022\u003Epredictive models to help find evil and improve outcomes for FireEye\u2019s customers and products\u003C\/a\u003E. If you are interested in this mission, please consider joining the team by \u003Ca href=\u0022https:\/\/www.fireeye.com\/company\/jobs.html\u0022\u003Eapplying to one of our job openings\u003C\/a\u003E.\u003C\/p\u003E\n",
        "jcr:lastModified": "Wed May 29 2019 10:29:27 GMT-0400",
        "sling:resourceType": "social\/blog\/components\/entrytext"
      }
    },
    "summary": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "text": "\u003Cp\u003EWe introduce a machine learning model that learns to rank strings based on their relevance for malware analysis.\u003C\/p\u003E\n",
      "jcr:lastModified": "Fri May 17 2019 13:54:40 GMT-0400",
      "sling:resourceType": "social\/blog\/components\/entrytextteaser"
    },
    "image": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "jcr:lastModified": "Wed May 29 2019 10:30:05 GMT-0400",
      "imageRotate": "0"
    }
  }
}
