{
  "jcr:primaryType": "cq:Page",
  "jcr:createdBy": "admin",
  "jcr:created": "Wed Mar 13 2019 12:07:32 GMT-0400",
  "jcr:content": {
    "jcr:primaryType": "cq:PageContent",
    "jcr:mixinTypes": [
      "mix:versionable"
    ],
    "jcr:createdBy": "admin",
    "jcr:title": "Breaking the Bank: Weakness in Financial AI Applications",
    "cq:lastReplicationAction": "Activate",
    "jcr:versionHistory": "7c8d2c70-1027-4b2b-a9b8-076bbb5cd9bf",
    "author": "Michelle Cantos",
    "cq:template": "\/apps\/fireeye-blog\/templates\/page_blogpost",
    "cq:lastReplicatedBy": "adam.greenberg@fireeye.com",
    "jcr:language": "en_us",
    "jcr:predecessors": [
      "e06e2aab-928d-47b8-9a0a-f8d0853621a4"
    ],
    "jcr:created": "Wed Mar 13 2019 12:07:32 GMT-0400",
    "cq:lastReplicated": "Wed Mar 13 2019 12:07:30 GMT-0400",
    "cq:lastModified": "Wed Mar 13 2019 12:07:21 GMT-0400",
    "jcr:baseVersion": "e06e2aab-928d-47b8-9a0a-f8d0853621a4",
    "jcr:isCheckedOut": true,
    "cq:tags": [
      "fireeye-blog-authors:michelle-cantos",
      "fireeye-blog-threat-research:threat-research",
      "fireeye-blog-tags:AI",
      "fireeye-blog-tags:artificial-intelligence",
      "fireeye-blog-tags:financial-services",
      "fireeye-blog-tags:homepage-carousel",
      "fireeye-blog-tags:latest"
    ],
    "jcr:uuid": "d72813f6-1583-4be4-93e0-062cead19ef5",
    "sling:resourceType": "social\/blog\/components\/page",
    "published": "Wed Mar 13 2019 12:00:00 GMT-0400",
    "cq:lastModifiedBy": "adam.greenberg@fireeye.com",
    "par": {
      "jcr:primaryType": "nt:unstructured",
      "sling:resourceType": "foundation\/components\/parsys",
      "entry": {
        "jcr:primaryType": "nt:unstructured",
        "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
        "text": "\u003Cp\u003ECurrently, threat actors possess limited access to the technology required to conduct disruptive operations against financial artificial intelligence (AI) systems and the risk of this targeting type remains low. However, there is a high risk of threat actors leveraging AI as part of disinformation campaigns to cause financial panic. As AI financial tools become more commonplace, adversarial methods to exploit these tools will also become more available, and operations targeting the financial industry will be increasingly likely in the future.\u003C\/p\u003E\n\u003Ch3\u003EAI Compounds Both Efficiency and Risk\u003C\/h3\u003E\n\u003Cp\u003EFinancial entities increasingly rely on AI-enabled applications to streamline daily operations, assess client risk, and detect insider trading. However, researchers have demonstrated how exploiting vulnerabilities in certain AI models can adversely affect the final performance of a system. Cyber threat actors can potentially leverage these weaknesses for financial disruption or economic gain in the future.\u003C\/p\u003E\n\u003Cp\u003ERecent advances in adversarial AI research highlights the vulnerabilities in some AI techniques used by the financial sector. \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1808.08994.pdf\u0022\u003EData poisoning attacks\u003C\/a\u003E, or manipulating a model\u0027s training data, can affect the end performance of a system by leading the model to generate inaccurate outputs or assessments. Manipulating the data used to train a model can be particularly powerful if it remains undetected, since \u0026quot;finished\u0026quot; models are often trusted implicitly. It should be noted that adversarial AI research demonstrates how anomalies in a model do not necessarily point users toward a wrong answer, but redirect users away from the more correct output. Additionally some cases of compromise require threat actors to obtain a copy of the model itself, through reverse engineering or compromising the machine learning pipeline of the target. The following are some vulnerabilities that assume this white-box knowledge of the models under attack:\u003C\/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EClassifiers are used for detection and identification, such as object recognition in driverless cars and malware detection in networks. Researchers have \u003Ca adhocenable=\u0022false\u0022 href=\u0022https:\/\/arxiv.org\/pdf\/1705.07263.pdf\u0022\u003Edemonstrated\u003C\/a\u003E how these \u003Ca adhocenable=\u0022false\u0022 href=\u0022https:\/\/www.utdallas.edu\/~muratk\/courses\/dmsec_files\/srndic-laskov-sp2014.pdf\u0022\u003Eclassifiers\u003C\/a\u003E can be susceptible to \u003Ca adhocenable=\u0022false\u0022 href=\u0022https:\/\/arxiv.org\/pdf\/1509.07892.pdf\u0022\u003Eevasion\u003C\/a\u003E, meaning objects can be misclassified due to inherent weaknesses in the mode (Figure 1).\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Cp style=\u0022margin-left: 40px;\u0022\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/breakingbank\/Picture1.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 1: Examples of classifier evasion where AI models identified 6 as 2\u003C\/span\u003E\u003C\/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EResearchers \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1808.05760.pdf\u0022\u003Ehave highlighted how data poisoning\u003C\/a\u003E can influence the outputs of AI recommendation systems. By changing reward pathways, adversaries can make a model suggest a suboptimal output such as reckless trades resulting in substantial financial losses. Additionally, groups \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1804.00792.pdf\u0022\u003Ehave demonstrated\u003C\/a\u003E a data-poisoning attack where attackers did not have control over how the training data was labeled.\u003C\/li\u003E\n\u003Cli\u003ENatural language processing applications can analyze text and generate a basic understanding of the opinions expressed, also known as sentiment analysis. \u003Ca href=\u0022https:\/\/pdfs.semanticscholar.org\/539f\/7b09681c614bd980b25f3054a79c240d504a.pdf\u0022\u003ERecent\u003C\/a\u003E \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1804.07998.pdf\u0022\u003Epapers\u003C\/a\u003E highlight how users can input corrupt text training examples into sentiment analysis models to degrade the model\u0027s overall performance and guide it to misunderstand a body of text.\u003Ci\u003E\u003C\/i\u003E\u003C\/li\u003E\n\u003Cli\u003ECompromises can also occur when the threat actor has limited access and understanding of the model\u2019s inner-workings. Researchers have demonstrated how \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1602.02697.pdf\u0022\u003Eopen access to the prediction functions\u003C\/a\u003E of a model as well as \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1605.07277.pdf\u0022\u003Eknowledge transfer\u003C\/a\u003E can also facilitate compromise.\u003Ci\u003E\u003C\/i\u003E\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Ch3\u003EHow Financial Entities Leverage AI\u003C\/h3\u003E\n\u003Cp\u003EAI can process large amounts of information very quickly, and financial institutions are adopting AI-enabled tools to make accurate risk assessments and streamline daily operations. As a result, threat actors likely view \u003Ca href=\u0022http:\/\/www.fsb.org\/wp-content\/uploads\/P011117.pdf\u0022\u003Efinancial service AI tools\u003C\/a\u003E as an attractive target to facilitate economic gain or financial instability (Figure 2).\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/breakingbank\/Picture2.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 2: Financial AI tools and their weaknesses\u003C\/span\u003E\u003C\/p\u003E\n\u003Ch5\u003ESentiment Analysis\u003C\/h5\u003E\n\u003Cp\u003E\u003Cu\u003EUse\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EBranding and reputation are variables that help analysts plan future trade activity and examine potential risks associated with a business. News and online discussions offer a wealth of resources to \u003Ca href=\u0022https:\/\/www.techemergence.com\/ai-for-sentiment-analysis-in-finance\/\u0022\u003Eexamine public sentiment\u003C\/a\u003E. AI techniques, such as natural language processing, can help analysts quickly identify public discussions referencing a business and examine the sentiment of these conversations to inform trades or help assess the risks associated with a firm.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EPotential Exploitation\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EThreat actors can potentially insert fraudulent data that could generate erroneous analyses regarding a publicly traded firm. For example, threat actors could distribute false negative information about a company that could have adverse effects on a business\u0027 future trade activity or lead to a damaging risk assessment. Manipulating the data used to train a model can be particularly powerful if it remains undetected, since \u0026quot;finished\u0026quot; models are often trusted implicitly.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EThreat Actors Using Disinformation to Cause Financial Panic\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EFireEye assess with high confidence that there is a high risk of threat actors\u0026nbsp;spreading false information that triggers AI enabled trading and causes financial panic. Additionally,\u0026nbsp;threat actors can leverage AI techniques to generate manipulated multimedia or \u0026quot;deep fakes\u0026quot; to facilitate such disruption.\u003C\/p\u003E\n\u003Cp\u003EFalse information can have considerable market-wide effects. Malicious actors have a history of distributing false information to facilitate financial instability. For example, in April 2013, the \u003Ca href=\u0022https:\/\/www.washingtonpost.com\/news\/worldviews\/wp\/2013\/04\/23\/syrian-hackers-claim-ap-hack-that-tipped-stock-market-by-136-billion-is-it-terrorism\/?utm_term=.cf45bbf40b22\u0022\u003ESyrian Electronic Army\u003C\/a\u003E (SEA) compromised the Associated Press (AP) Twitter account and announced that the White House was attacked and President Obama sustained injuries. After the false information was posted, stock prices plummeted.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/breakingbank\/Picture3.png\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 3: Tweet from the Syrian Electronic Army (SEA) after compromising Associated Press\u0027s Twitter account\u003C\/span\u003E\u003C\/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EMalicious actors distributed false messaging that triggered bank runs in \u003Ca href=\u0022https:\/\/www.economist.com\/eastern-approaches\/2014\/07\/02\/why-the-run-on-banks\u0022\u003EBulgaria\u003C\/a\u003E and \u003Ca href=\u0022https:\/\/www.rferl.org\/a\/kazakhstan-bank-run-social-media\/25268463.html\u0022\u003EKazakhstan\u003C\/a\u003E in 2014. In two separate incidents, criminals sent emails, text messages, and social media posts suggesting bank deposits were not secure, causing customers to withdraw their savings en masse.\u003C\/li\u003E\n\u003Cli\u003EThreat actors can use AI to create manipulated multimedia videos or \u0026quot;\u003Ca href=\u0022https:\/\/www.technologyreview.com\/s\/611810\/fake-america-great-again\/\u0022\u003Edeep fakes\u003C\/a\u003E\u0026quot; to spread false information about a firm or market-moving event. Threat actors can also use AI applications to replicate the voice of a company\u0027s leadership to conduct fraudulent trades for financial gain.\u003C\/li\u003E\n\u003Cli\u003EWe have observed one \u003Ca href=\u0022https:\/\/www.theguardian.com\/world\/2018\/mar\/13\/muslim-cyber-army-a-fake-news-operation-designed-to-bring-down-indonesias-leader\u0022\u003Eexample where\u003C\/a\u003E a manipulated video likely impacted the outcome of a political campaign.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Ch5\u003EPortfolio Management\u003C\/h5\u003E\n\u003Cp\u003E\u003Cu\u003EUse\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003ESeveral financial institutions are employing AI applications to \u003Ca href=\u0022https:\/\/www.bloomberg.com\/news\/articles\/2017-11-16\/self-learning-algorithm-to-pick-stocks-for-first-nordic-ai-fund\u0022\u003Eselect stocks for investment funds\u003C\/a\u003E, or in the case of \u003Ca href=\u0022https:\/\/www.wired.com\/2016\/01\/the-rise-of-the-artificially-intelligent-hedge-fund\/\u0022\u003EAI-based hedge funds\u003C\/a\u003E, \u003Ca href=\u0022https:\/\/www.bloomberg.com\/news\/features\/2017-09-27\/the-massive-hedge-fund-betting-on-ai\u0022\u003Eautomatically conduct trades\u003C\/a\u003E to maximize profits. Financial institutions can also leverage AI applications to help customize a client\u0027s trade portfolio. AI applications can analyze a client\u0027s previous trade activity and propose future trades analogous to those already found in a client\u0027s portfolio.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EPotential Exploitation\u003Cb\u003E\u003C\/b\u003E\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EActors could influence recommendation systems to redirect a hedge fund toward irreversible bad trades, causing the company to lose money (e.g., flooding the market with trades that can confuse the recommendation system and cause the system to start trading in a way that damages the company).\u003C\/p\u003E\n\u003Cp\u003EMoreover, many of the automated trading tools used by hedge funds operate without human supervision and conduct trade activity that directly affects the market. This lack of oversight could leave future automated applications more vulnerable to exploitation as there is no human in the loop to detect anomalous threat activity.\u003Cb\u003E\u003C\/b\u003E\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EThreat Actors Conducting Suboptimal Trades\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EWe assess with moderate confidence that manipulating trade recommendation systems poses a moderate risk to AI-based portfolio managers.\u003C\/p\u003E\n\u003Cul style=\u0022list-style-position: inside;\u0022\u003E\n\u003Cli\u003EThe diminished human involvement with trade recommendation systems coupled with the irreversibility of trade activity suggest that adverse recommendations could quickly escalate to a large-scale impact.\u003C\/li\u003E\n\u003Cli\u003EAdditionally, operators can influence recommendation systems without access to sophisticated AI technologies; instead, using knowledge of the market and mass trades to degrade the application\u0027s performance.\u003C\/li\u003E\n\u003Cli\u003EWe have previously observed malicious actors targeting trading platforms and exchanges, as well as compromising bank networks to conduct manipulated trades.\u003C\/li\u003E\n\u003Cli\u003EBoth state-sponsored and financially motivated actors have incentives to exploit automated trading tools to generate profit, destabilize markets, or weaken foreign currencies.\u003C\/li\u003E\n\u003Cli\u003ERussian hackers reportedly leveraged \u003Ca href=\u0022https:\/\/thehackernews.com\/2016\/02\/russian-exchange-hacked.html\u0022\u003ECorkow malware\u003C\/a\u003E to place $500M worth of trades at non-market rates, briefly destabilizing the dollar-ruble exchange rate in February 2015. Future criminal operations can leverage vulnerabilities in automatic training algorithms to disrupt the market with a flood of automated bad trades.\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Ch5\u003ECompliance and Fraud Detection\u003C\/h5\u003E\n\u003Cp\u003E\u003Cu\u003EUse\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EFinancial institutions and regulators are leveraging AI-enabled \u003Ca href=\u0022https:\/\/www.americanbanker.com\/news\/human-please-look-at-this-nasdaq-using-ai-to-spot-abuses\u0022\u003Eanomaly detection tools\u003C\/a\u003E to ensure that traders are not engaging in illegal activity. These tools can examine trade activity, \u003Ca href=\u0022https:\/\/www.businessinsider.com\/wall-street-banks-use-artificial-intelligence-to-monitor-traders-2016-3\u0022\u003Einternal communications\u003C\/a\u003E, and \u003Ca href=\u0022https:\/\/www.cnbc.com\/2017\/03\/27\/behavox-rogue-traders-compliance-artificial-intelligence.html\u0022\u003Eother employee data\u003C\/a\u003E to ensure that workers are not capitalizing on advanced knowledge of the market to engage in fraud, theft, insider trading, or embezzlement.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EPotential Exploitation\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003ESophisticated threat actors can exploit the weaknesses in classifiers to alter an AI-based detection tool and mischaracterize anomalous illegal activity as normal activity. Manipulating the model helps insider threats conduct criminal activity without fear of discovery.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EThreat Actors Camouflaging Insider Threat Activity\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003ECurrently threat actors possess limited access to the kind of technology required to evade these fraud detection systems, and therefore with high confidence we assess that the threat of this activity type remains low. However, as AI financial tools become more commonplace, adversarial methods to exploit these tools will also become more available and insider threats leveraging AI to evade detection will likely increase in the future.\u003C\/p\u003E\n\u003Cp\u003EUnderground forums and social media posts demonstrate there is a market for individuals with insider access to financial institutions. Insider threats could exploit weaknesses in AI-based anomaly detectors to camouflage nefarious activity, such as external communications, erratic trades, and data transfers, as normal activity.\u003C\/p\u003E\n\u003Ch5\u003ETrade Simulation\u003C\/h5\u003E\n\u003Cp\u003E\u003Cu\u003EUse\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EFinancial entities can use AI tools that leverage historical data from previous trade activity to simulate trades and examine their effects. Quant-fund managers and high-speed traders can use this capability to strategically plan future activity, such as the optimal time of the day to trade. Additionally, financial insurance underwriters can use these tools to observe the impact of market-moving activity and generate better risk assessments.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EPotential Exploitation\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EBy exploiting inherent weaknesses in an AI model, threat actors could lull a company into a false sense of security regarding the way a trade will play out. Specifically, threat actors could find out when a company is training their model and inject corrupt data into a dataset being used to train the model. Subsequently, the end application generates an incorrect simulation of potential trades and their consequences. These models are regularly trained on the latest financial information to improve a simulation\u0027s performance, providing threat actors with multiple opportunities for data poisoning attacks. Additionally, some high-speed traders speculate that threats could \u003Ca href=\u0022https:\/\/www.wsj.com\/articles\/pentagon-turns-to-high-speed-traders-to-fortify-markets-against-cyberattack-1508065202\u0022\u003Eflood the market with fake sell orders to confuse trading algorithms and potentially cause the market to crash\u003C\/a\u003E.\u003C\/p\u003E\n\u003Cp\u003EFireEye Threat Intelligence has previously examined how financially motivated actors can leverage data manipulation for profit through pump and dump scams and stock manipulation.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EThreat Actors Conducting Insider Trading\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EFireEye assesses with moderate confidence that the current risk of threat actors leveraging these attacks is low as exploitations of trade simulations require sophisticated technology as well as additional insider intelligence regarding when a financial company is training their model. Despite these limitations, as financial AI tools become more popular, adversarial methods to exploit these tools are also likely to become more commonplace on underground forums and via state-sponsored threats. Future financially motivated operations could monitor or manipulate trade simulation tools as another means of gaining advanced knowledge of upcoming market activity.\u003C\/p\u003E\n\u003Ch5\u003ERisk Assessment and Modeling\u003C\/h5\u003E\n\u003Cp\u003E\u003Cu\u003EUse\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EAI can help the financial insurance sector\u0027s underwriting process by \u003Ca href=\u0022https:\/\/www.bloomberg.com\/professional\/blog\/artificial-intelligence-game-changer-risk-management-finance\/\u0022\u003Eexamining\u003C\/a\u003E client data and highlighting features that it considers vulnerable prior to market-moving actions (joint ventures, mergers \u0026amp; acquisitions, research \u0026amp; development breakthroughs, etc.). Creating an accurate insurance policy ahead of market catalysts requires a risk assessment to highlight a client\u0027s potential weaknesses.\u003C\/p\u003E\n\u003Cp\u003EFinancial services can also employ AI applications to \u003Ca href=\u0022https:\/\/www.bloomberg.com\/professional\/blog\/artificial-intelligence-game-changer-risk-management-finance\/\u0022\u003Eimprove their risk models\u003C\/a\u003E. Advances in \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1406.2661.pdf\u0022\u003Egenerative adversarial networks\u003C\/a\u003E can \u003Ca href=\u0022https:\/\/towardsdatascience.com\/using-bidirectional-generative-adversarial-networks-to-estimate-value-at-risk-for-market-risk-c3dffbbde8dd\u0022\u003Ehelp risk management\u003C\/a\u003E by stress-testing a firm\u0027s internal risk model to evaluate performance or highlight potential vulnerabilities in a firm\u0027s model.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EPotential Exploitation\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EIf a country is conducting market-moving events with a foreign business, state-sponsored espionage actors could use data poisoning attacks to cause AI models to over or underestimate the value or risk associated with a firm to gain a competitive advantage ahead of planned trade activity. For example, espionage actors could feasibly use this knowledge and help turn a joint venture into a hostile takeover or eliminate a competitor in a bidding process. Additionally, threat actors can exploit weaknesses in financial AI tools as part of larger third-party compromises against high-value clients.\u003C\/p\u003E\n\u003Cp\u003E\u003Cu\u003EThreat Actors Influencing Trade Deals and Negotiations\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EWith high confidence, we consider the current threat risk to trade activity and business deals to be low, but as more companies leverage AI applications to help prepare for market-moving catalysts, these applications will likely become an attack surface for future espionage operations.\u003C\/p\u003E\n\u003Cp\u003EIn the past, state-sponsored actors have employed espionage operations during collaborations with foreign companies to ensure favorable business deals. Future state-sponsored espionage activity could leverage weaknesses in financial modeling tools to help nations gain a competitive advantage.\u003C\/p\u003E\n\u003Ch3\u003EOutlook and Implications\u003C\/h3\u003E\n\u003Cp\u003EBusinesses adopting AI applications should be aware of the risks and vulnerabilities introduced with these technologies, as well as the potential benefits. It should be noted that AI models are not static; they are routinely updated with new information to make them more accurate. This constant model training frequently leaves them vulnerable to manipulation. Companies should remain vigilant and regularly audit their training data to \u003Ca href=\u0022https:\/\/www.utdallas.edu\/~muratk\/courses\/dmsec_files\/rpca_imc09.pdf\u0022\u003Eeliminate\u003C\/a\u003E \u003Ca href=\u0022http:\/\/mmnet.iis.sinica.edu.tw\/botnet\/file\/20101011\/20101011_1.pdf\u0022\u003Epoisoned inputs.\u003C\/a\u003E Additionally, where applicable, AI applications should incorporate human supervision to ensure that erroneous outputs or recommendations do not automatically result in financial disruption.\u003C\/p\u003E\n\u003Cp\u003EAI\u0027s inherent limitations also pose a problem as the financial sector increasingly adopts these applications for their operations. The \u003Ca href=\u0022https:\/\/ctml.psu.edu\/\u0022\u003Elack of transparency\u003C\/a\u003E in how a model arrived at its answer is problematic for analysts who are using AI recommendations to conduct trades. Without an \u003Ca href=\u0022https:\/\/www.camlis.org\/awalin-nabila-sopan\/\u0022\u003Eexplanation for its output\u003C\/a\u003E, it is difficult to determine liability when a trade has negative outcomes. This \u003Ca href=\u0022https:\/\/papers.nips.cc\/paper\/7798-to-trust-or-not-to-trust-a-classifier.pdf\u0022\u003Elack of clarity\u003C\/a\u003E can lead analysts to \u003Ca href=\u0022https:\/\/www.darpa.mil\/program\/explainable-artificial-intelligence\u0022\u003Emistrust an application\u003C\/a\u003E and eventually refrain from using it altogether. Additionally, the rise of data privacy laws may also accelerate the need for explainable AI in the financial sector. Europe\u0027s \u003Ca href=\u0022https:\/\/www.privacy-regulation.eu\/en\/r71.htm\u0022\u003EGeneral Data Protection Regulation (GDPR)\u003C\/a\u003E stipulates that companies employing AI applications must have an \u003Ca href=\u0022http:\/\/fortune.com\/2018\/05\/25\/ai-machine-learning-privacy-gdpr\/\u0022\u003Eexplanation for decisions made by its models\u003C\/a\u003E.\u003C\/p\u003E\n\u003Cp\u003ESome financial institutions have begun addressing this explainability problem by \u003Ca href=\u0022https:\/\/www.technologyreview.com\/s\/604122\/the-financial-world-wants-to-open-ais-black-boxes\/\u0022\u003Edeveloping AI models\u003C\/a\u003E that are inherently more transparent. Researchers have also developed \u003Ca href=\u0022http:\/\/people.csail.mit.edu\/davidam\/docs\/SENN.pdf\u0022\u003Eself-explaining neural networks\u003C\/a\u003E, which provide understandable explanations for the outputs generated by the system.\u003C\/p\u003E\n",
        "jcr:lastModified": "Tue Mar 12 2019 19:52:34 GMT-0400",
        "sling:resourceType": "social\/blog\/components\/entrytext"
      }
    },
    "image": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "jcr:lastModified": "Wed Mar 13 2019 12:07:21 GMT-0400",
      "imageRotate": "0"
    },
    "summary": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "text": "\u003Cp\u003EThe more common AI financial tools become, the more threat actors will exploit them.\u003C\/p\u003E\n",
      "jcr:lastModified": "Tue Mar 12 2019 19:57:19 GMT-0400",
      "sling:resourceType": "social\/blog\/components\/entrytextteaser"
    }
  }
}
