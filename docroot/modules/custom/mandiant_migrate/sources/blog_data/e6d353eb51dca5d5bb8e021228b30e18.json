{
  "jcr:primaryType": "cq:Page",
  "jcr:createdBy": "admin",
  "jcr:created": "Thu Jan 21 2021 17:39:20 GMT+0000",
  "jcr:content": {
    "jcr:primaryType": "cq:PageContent",
    "jcr:mixinTypes": [
      "mix:versionable"
    ],
    "jcr:createdBy": "admin",
    "jcr:title": "Training Transformers for Cyber Security Tasks: A Case Study on Malicious URL Prediction",
    "jcr:versionHistory": "955dcf3a-d380-4851-b92e-0fe4757b1a5d",
    "author": "Ethan M. Rudd",
    "cq:template": "\/apps\/fireeye-blog\/templates\/page_blogpost",
    "jcr:language": "en_us",
    "jcr:predecessors": [
      "c2682625-331a-4ee3-9a9d-5752f077adc9"
    ],
    "jcr:created": "Thu Jan 21 2021 17:39:20 GMT+0000",
    "cq:lastModified": "Thu Jan 21 2021 17:34:05 GMT+0000",
    "jcr:baseVersion": "c2682625-331a-4ee3-9a9d-5752f077adc9",
    "jcr:isCheckedOut": true,
    "cq:tags": [
      "fireeye-blog-authors:ethan-rudd",
      "fireeye-blog-authors:ahmed-abdallah",
      "fireeye-blog-threat-research:threat-research",
      "fireeye-blog-tags:data-science",
      "fireeye-blog-tags:homepage-carousel",
      "fireeye-blog-tags:latest",
      "fireeye-blog-tags:machine-learning",
      "fireeye-blog-tags:url"
    ],
    "jcr:uuid": "03b6e2c5-c312-4af5-a0da-e768fc274155",
    "sling:resourceType": "social\/blog\/components\/page",
    "published": "Thu Jan 21 2021 12:30:00 GMT-0500",
    "cq:lastModifiedBy": "adam.greenberg@fireeye.com",
    "par": {
      "jcr:primaryType": "nt:unstructured",
      "sling:resourceType": "foundation\/components\/parsys",
      "entry": {
        "jcr:primaryType": "nt:unstructured",
        "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
        "text": "\u003Ch4\u003EHighlights\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;\u003C\/h4\u003E\n\u003Cul\u003E\n\u003Cli\u003EPerform a case study on using Transformer models to solve cyber security problems\u003C\/li\u003E\n\u003Cli\u003ETrain a Transformer model to detect malicious URLs under multiple training regimes\u003C\/li\u003E\n\u003Cli\u003ECompare our model against other deep learning methods, and show it performs on-par with other top-scoring models\u003C\/li\u003E\n\u003Cli\u003EIdentify issues with applying generative pre-training to malicious URL detection, which is a cornerstone of Transformer training in natural language processing (NLP) tasks\u003C\/li\u003E\n\u003Cli\u003EIntroduce novel loss function that balances classification and generative loss to achieve improved performance on the malicious URL detection task\u003C\/li\u003E\n\u003C\/ul\u003E\n\u003Ch4\u003EIntroduction\u003C\/h4\u003E\n\u003Cp\u003EOver the past three years Transformer machine learning (ML) models, or \u201cTransformers\u201d for short, have yielded impressive breakthroughs in a variety of sequence modeling problems, specifically natural language processing (NLP). For example, \u003Ca href=\u0022https:\/\/arxiv.org\/abs\/2005.14165\u0022\u003EOpenAI\u2019s latest\u003C\/a\u003E GPT-3 model is capable of generating long segments of grammatically-correct prose from scratch. Spinoff models, such as those developed for question and answering, are capable of correlating context over multiple sentences. \u003Ca href=\u0022https:\/\/play.aidungeon.io\/main\/landing\u0022\u003EAI Dungeon\u003C\/a\u003E, a single and multiplayer text adventure game, uses Transformers to generate plausible unlimited content in a variety of fantasy settings. Transformers\u2019 NLP modeling capabilities are apparently so powerful that they pose security risks in their own right, in terms of their \u003Ca href=\u0022https:\/\/www.theatlantic.com\/ideas\/archive\/2020\/09\/future-propaganda-will-be-computer-generated\/616400\/\u0022\u003Epotential power to spread disinformation\u003C\/a\u003E, yet on the other side of the coin, they can be used as powerful tools to detect and mitigate disinformation campaigns. For example, in \u003Ca href=\u0022\/content\/fireeye-www\/en_US\/blog\/threat-research\/2019\/11\/combatting-social-media-information-operations-neural-language-models.html\u0022 adhocenable=\u0022false\u0022\u003Eprevious research\u003C\/a\u003E by the FireEye Data Science team, a NLP Transformer was fine-tuned to detect disinformation on social media sites.\u003C\/p\u003E\n\u003Cp\u003EGiven the power of these Transformer models, it seems natural to wonder if we can apply them to other types of cyber security problems that do not necessarily involve natural language, per se. In this blog post, we discuss a case study in which we apply Transformers to malicious URL detection. Studying Transformer performance on URL detection problem is a first logical step to extending Transformers to more generic cyber security tasks, since URLs are not technically natural language sequences but share some common characteristics with NLP.\u003C\/p\u003E\n\u003Cp\u003EIn the following sections, we outline a typical Transformer architecture and discuss how we adapt it to URLs with a character-focused tokenization. We then discuss loss functions we employ to guide the training of the model, and finally compare our training approaches to more conventional ML-based modeling options.\u003C\/p\u003E\n\u003Ch4\u003EAdapting Transformers to URLs\u003C\/h4\u003E\n\u003Cp\u003EOur URL Transformer operates at the character level, where each character in the URL corresponds to an input token. When a URL is input to our Transformer, it is appended with special tokens\u2014a classification token (\u201cCLS\u201d) that conditions the model to produce a prediction and padding tokens (\u201cPAD\u201d) that normalize the input to a fixed length to allow for parallel training. Each token in the input string is then projected into a character embedding space, followed by a stack of \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Attention_(machine_learning)\u0022\u003EAttention\u003C\/a\u003E and \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Feedforward_neural_network\u0022\u003EFeed-Forward Neural Network (FFNN)\u003C\/a\u003E layers. This stack of layers is similar to the architecture introduced in the \u003Ca href=\u0022https:\/\/papers.nips.cc\/paper\/2017\/file\/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\u0022\u003Eoriginal Transformers paper\u003C\/a\u003E. At a high level, the Attention layers allow each input to be associated with long-distance context of other characters that are important for the classification task, similar to the notion of attention in humans, while the FFNN layers provide capacity for learning the relationships among the combination of inputs and their respective contexts. An illustration of our architecture is shown in Figure 1.\u003Cu\u003E\u003C\/u\u003E\u003C\/p\u003E\n\u003Cp\u003EAdditionally, the URL Transformer employs a masking strategy in its Attention calculation, which enforces a left-to-right (L-R) dependence. This means that only input characters from the left of a given character influence that character\u2019s representation in each layer of the attention stack. The network outputs one embedding for each input character, which captures all information learned by the model about the character sequence up to that point in the input.\u003C\/p\u003E\n\u003Cp\u003EOnce the model is trained, we can use the URL Transformer to perform several different tasks, such as generatively predicting the next character in the input sequence by using the sequence embedding () as an input to another neural network with as softmax output over the possible vocabulary of characters. A specific example of this is shown in Figure 1, where we take the embedding of the input \u201cfiree\u201d() and use it to predict the next most likely character, \u201cy.\u201d Similarly, we can use the embedding produced after the classification token to predict other properties of the input sequences, such as their likelihood of maliciousness.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/mal-url-prediction\/fig1.png\u0022 alt=\u0022\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 1: High-level overview of the URL Transformer architecture\u003C\/span\u003E\u003C\/p\u003E\n\u003Ch4\u003ELoss Functions and Training Regimes\u003C\/h4\u003E\n\u003Cp\u003EWith the model architecture in hand, we now turn to the question of how we train the model to most effectively detect malicious URLs. Of course, we can train this model in a similar way to other supervised deep learning classifiers by: (1) making predictions on samples from a labeled training set, (2) using a \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Loss_function\u0022\u003Eloss function\u003C\/a\u003E to measure the quality of our predictions, and (3) tune model parameters (i.e., weights) via \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Backpropagation\u0022\u003Ebackpropagation\u003C\/a\u003E. However, the nature of the Transformer model allows for several interesting variations to this \u003Ci\u003Etraining regime\u003C\/i\u003E. In fact, one of the reasons that Transformers have become so popular for NLP tasks is because they allow for \u003Ca href=\u0022https:\/\/en.wikipedia.org\/wiki\/Semi-supervised_learning\u0022\u003Eself-supervised\u003C\/a\u003E generative pre-training, which takes advantage of massive amounts of unlabeled data to help the model learn general characteristics of the input language before being fine-tuned on the ultimate task at-hand (e.g., question answering, sentiment analysis, etc.). Here, we outline some of the training regimes we explored for our URL Transformer model.\u003C\/p\u003E\n\u003Ch5\u003EDirect Label Prediction (Decode-To-Label)\u003C\/h5\u003E\n\u003Cp\u003EUsing a training set of URLs with malicious and benign labels, we can treat the URL Transformer architecture as a feature extractor, whose outputs we use as the input to a traditional classifier (e.g., FFNN or even a random forest). When using a FFNN as our classifier, we can backpropagate the classification loss (e.g., binary cross-entropy) through both the classifier and the Transformer network to adjust the weights to perform classification. This training regime is the baseline for our experiments and is how most deep learning models are trained for classification tasks.\u003C\/p\u003E\n\u003Ch5\u003ENext-Character Prediction Pre-Training and Fine-Tuning\u003C\/h5\u003E\n\u003Cp\u003EBeyond the baseline classification training regime, the NLP literature suggests that one can learn a self-supervised embedding of the input sequence by training the Transformer to perform a next-character prediction task, then fine-tuning the learned representation for the classification problem. A key advantage of this approach is that data used for pre-training does not require malicious or benign labels; instead, the next characters in a URL serve as the labels to be predicted from prior characters in the sequence. This is similar to the example given in Figure 1, where the embedding output is used to predict the next character, \u201cy,\u201d in \u201cfireeye.com.\u201d Overall, this training regime allows us to take advantage of the massive amount of unlabeled data that is typically available in cyber security-related problems.\u003C\/p\u003E\n\u003Cp\u003EThe overall structure of the architecture for this regime is similar to the aforementioned binary classification task, with FFNN layers added for classification. However, since we are now predicting multiple classes (i.e., one class per input character in the vocabulary), we must apply a softmax function to the output to induce a probability distribution over the potential output characters. Once the Transformer portion of the network is pre-trained in this way, we can swap the FFNN classification layers focused on character prediction with new layers that will be trained for the malicious URL classification problem, as in the decode-to-label case.\u003C\/p\u003E\n\u003Ch5\u003EBalanced Mixed-Objective Training\u003C\/h5\u003E\n\u003Cp\u003E\u003Ca href=\u0022https:\/\/www.usenix.org\/system\/files\/sec19-rudd.pdf\u0022\u003EPrior work\u003C\/a\u003E has shown that imbuing the training process with additional knowledge outside of the primary task can help constrain the learning process, and ultimately result in better models. For instance, a malware classifier might train using loss functions that capture malicious\/benign classification, malware family prediction, and tag prediction tasks as a mechanism to provide the classifier with broader understanding of the problem than looking at malicious\/benign labels in isolation.\u003C\/p\u003E\n\u003Cp\u003EInspired by these findings, we also introduced a mixed-objective training regime for our URL Transformer, where we train for binary classification and next-character prediction simultaneously. At each iteration of training, we compute a loss multiplier such that each loss contribution is fixed prior to backpropagation. This ensures that neither loss term dominates during training. Specifically, for minibatch \u003Ci\u003Ei\u003C\/i\u003E, let the net loss \u003Ci\u003EL\u003Cspan class=\u0022type-XS\u0022\u003EMixed\u003C\/span\u003E\u003C\/i\u003E be computed as follows:\u003C\/p\u003E\n\u003Cp style=\u0022text-align: center;\u0022\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/mal-url-prediction\/lmixed.png\u0022 style=\u0022width: 40%; height: 40%\u0022 alt=\u0022\u0022\u003E\u0026nbsp;\u003C\/p\u003E\n\u003Cp\u003EGiven hyperparameters \u003Ci style=\u0022font-size: 12px;\u0022\u003Ea\u003C\/i\u003E and \u003Ci style=\u0022font-size: 12px;\u0022\u003Eb\u003C\/i\u003E, defined such that \u003Ci style=\u0022font-size: 12px;\u0022\u003Ea\u003C\/i\u003E + \u003Ci style=\u0022font-size: 12px;\u0022\u003Eb\u003C\/i\u003E: = 1, we compute constant \u003Ci style=\u0022font-size: 12px;\u0022\u003Ea\u003C\/i\u003E so that the net loss contribution of \u003Ci style=\u0022font-size: 12px;\u0022\u003EL\u003Cspan class=\u0022type-XS\u0022\u003ECLS\u003C\/span\u003E\u003C\/i\u003E to \u003Ci style=\u0022font-size: 12px;\u0022\u003EL\u003Cspan class=\u0022type-XS\u0022\u003EMixed\u003C\/span\u003E\u003C\/i\u003E is \u003Ci style=\u0022font-size: 12px;\u0022\u003Ea\u003C\/i\u003E and the net contribution of \u003Ci style=\u0022font-size: 12px;\u0022\u003EL\u003Cspan class=\u0022type-XS\u0022\u003ENext\u003C\/span\u003E\u003C\/i\u003E to \u003Ci style=\u0022font-size: 12px;\u0022\u003EL\u003Cspan class=\u0022type-XS\u0022\u003EMixed\u003C\/span\u003E\u003C\/i\u003E is \u003Ci style=\u0022font-size: 12px;\u0022\u003Eb\u003C\/i\u003E. For our evaluations, we set \u003Ci style=\u0022font-size: 12px;\u0022\u003Ea\u003C\/i\u003E := \u003Ci style=\u0022font-size: 12px;\u0022\u003Eb\u003C\/i\u003E := 0.5, effectively requiring that the model equally balance its ability to generate the next character and accurately predict malicious URLs.\u003C\/p\u003E\n\u003Ch4\u003EEvaluation\u003C\/h4\u003E\n\u003Cp\u003ETo evaluate our URL Transformer model and better understand the impact of the three training regimes discussed earlier, we collected a training dataset of over 1M labeled malicious and benign URLs, which was split into roughly 700K training samples, 100K validation samples, and 200k test samples. Additionally, we also developed an unlabeled pre-training dataset of 20M URLs.\u003C\/p\u003E\n\u003Cp\u003EUsing this data, we performed four different training runs for our Transformer model:\u003C\/p\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cb\u003EDecodeToLabel (Baseline): \u003C\/b\u003EUsing strictly the binary cross-entropy loss on the embedded classification features over the entire sequence, we trained the model for 15 epochs using the training set.\u003Cb\u003E\u003C\/b\u003E\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EMixedObjective: \u003C\/b\u003EWe trained the model for 15 epochs on the training set, using both the embedded classification features and the embedded next-character prediction features.\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EFineTune: \u003C\/b\u003EWe pre-trained the model for 15 epochs on the next-character prediction task using the training set, ignoring the malicious\/benign labels. We then froze weights over the first 16 layers of the model and trained the model for an additional 15 epochs using a binary cross-entropy loss on the classification labels.\u003C\/li\u003E\n\u003Cli\u003E\u003Cb\u003EFineTune 20M:\u003C\/b\u003E We performed pre-training on the next-character prediction task using the 20M URL dataset, pre-training for 2 epochs. We then froze weights over the first 16 layers of the Transformer and trained for 15 epochs on the binary classification task.\u003C\/li\u003E\n\u003C\/ol\u003E\n\u003Cp\u003EThe ROC curve shown in Figure 2 compares the performance of these four training regimes. Here, our baseline DecodeToLabel model (red) yielded a ROC curve with 0.9484 AUC, while the MixedObjective model (green) slightly outperformed the baseline with an AUC of 0.956. Interestingly, both of the fine-tuning models yielded poor classification results, which is counter to the established practice of these Transformer models in the NLP domain.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/mal-url-prediction\/fig2.png\u0022 alt=\u0022\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 2: ROC curves for four URL Transformer training regimes\u003C\/span\u003E\u003C\/p\u003E\n\u003Cp\u003ETo assess the relative efficacy of our Transformer models on this dataset, we also fit several other types of benchmark models developed for URL classification: (1) a Random Forest model on SME-derived features, (2) a 1D Convolutional Neural Network (CNN) model on character embeddings, and (3) a Long Short-Term Memory (LSTM) neural network on character embeddings. Details of these models can be found in our \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/2011.03040.pdf\u0022\u003Ewhite paper\u003C\/a\u003E, however we find that our top performing Transformer model performs on-par with the best performing non-Transformer baseline (a 1D CNN model), which perhaps indicates that the long-range dependencies typically learned by Transformer models are not as useful in the case of malicious URL detection.\u003C\/p\u003E\n\u003Cp\u003E\u003Cimg src=\u0022\/content\/dam\/fireeye-www\/blog\/images\/mal-url-prediction\/fig3.png\u0022 alt=\u0022\u0022\u003E\u003Cbr\u003E\n\u003Cspan class=\u0022type-XS\u0022\u003EFigure 3: ROC curves comparing URL Transformer to other benchmark URL classification models\u003C\/span\u003E\u003C\/p\u003E\n\u003Ch4\u003ESummary\u003C\/h4\u003E\n\u003Cp\u003EOur experiments suggest that Transformers can achieve performance comparable to or better than that of other top-performing models for URL classification, though the details of how to achieve that performance differ from common practice. Contrary to \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/1810.04805.pdf?source=post_elevate_sequence_page---------------------------\u0022\u003Efindings from the NLP domain\u003C\/a\u003E, wherein self-supervised pre-training substantially enhances performance in a fine-tuned classification task, similar pretraining approaches actually diminish performance for malicious URL detection. This suggests that the next character prediction task has too little apparent correlation with the task of malicious\/benign prediction for effective\/stable transfer.\u003C\/p\u003E\n\u003Cp\u003EInterestingly, utilizing next-character prediction as an auxiliary loss function in conjunction with a malicious\/benign loss yields improvements over training solely to predict the label. We hypothesize that while pre-training leads to a relatively poor generative model due to randomized content in the URLs within our dataset, a malicious\/benign loss may serve to better condition the generative model learned by the next-character prediction task, distilling a subset of relevant information. It may also be the case that the long-distance relationships that are key to the generative pre-training task are not as important for the final malicious URL classification, as evidenced by the performance of the 1D CNN model.\u003C\/p\u003E\n\u003Cp\u003ENote that we did not perform a rigorous hyperparameter search for our Transformer, since this research was primarily concerned with loss functions and training regimes. Therefore, it is still an open question as to whether a more optimal architecture, specifically designed for this classification task, could substantially outperform the models described here.\u003C\/p\u003E\n\u003Cp\u003EWhile our URL dataset is not representative of all data in the cyber security space, the difficulty of obtaining a readily fine-tuned model from self-supervised pre-training suggests that this approach is unlikely to work well for training Transformers on longer sequences or sequences with lesser resemblance to natural language (e.g., PE files), but an auxiliary loss might work.\u003C\/p\u003E\n\u003Cp\u003EDetails about this research and additional results can be found in our associated \u003Ca href=\u0022https:\/\/arxiv.org\/pdf\/2011.03040.pdf\u0022\u003Ewhite paper\u003C\/a\u003E.\u003C\/p\u003E\n",
        "jcr:lastModified": "Thu Jan 21 2021 17:34:05 GMT+0000",
        "sling:resourceType": "social\/blog\/components\/entrytext"
      }
    },
    "summary": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "text": "\u003Cp\u003ETransformer machine learning has seen big breakthroughs in recent years, and in this blog post we discuss a case study in which we apply Transformers to malicious URL detection.\u003C\/p\u003E\n",
      "jcr:lastModified": "Wed Jan 20 2021 19:19:12 GMT+0000",
      "sling:resourceType": "social\/blog\/components\/entrytextteaser"
    },
    "image": {
      "jcr:primaryType": "nt:unstructured",
      "jcr:lastModifiedBy": "adam.greenberg@fireeye.com",
      "jcr:lastModified": "Thu Jan 21 2021 17:28:55 GMT+0000",
      "imageRotate": "0"
    }
  }
}
